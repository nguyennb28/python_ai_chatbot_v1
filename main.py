from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

template = """
Answer the question below.

Here is the conversation history: {context}

Question: {question}

Answer:
"""

# Video youtube: https://www.youtube.com/watch?v=d0o89z134CQ

model = OllamaLLM(model="llama3.2:3b")
prompt = ChatPromptTemplate.from_template(template)
chain = prompt | model

def handle_conversation():
    context = "You are Ms.Quá»³nh Vy and you are a super good English teacher in Vietnam."
    print("ChÃ o báº¡n ğŸ™Œ, tÃ´i lÃ  giÃ¡o viÃªn dáº¡y tiáº¿ng anh, tÃ´i cÃ³ thá»ƒ giÃºp gÃ¬ cho báº¡n. \n GÃµ 'exit' Ä‘á»ƒ thoÃ¡t.")
    while True:
        user_input = input("You: ")
        if user_input.lower() == "exit":
            print("Ráº¥t vui khi Ä‘Æ°á»£c Ä‘á»“ng hÃ nh cÃ¹ng báº¡n ğŸ˜")
            break
        
        
        result = chain.invoke({"context": context, "question": user_input})
        print("Bot: ", result)
        context += f"\nUser: {user_input}\nAI: {result}"

# print(handle_conversation())
if __name__ == "__main__":
    handle_conversation()
